

## Data

### what is data?

data is a collection of data objects and their attributes. An attribute is a property or characteristic of an object and a collections of attributes describe an object.



### Types of data sets

- record
- graph
- ordered

### types of attributes and properties

| type     | properties                                  |
| -------- | ------------------------------------------- |
| nominal  | distinctness                                |
| ordinal  | distinctness, order                         |
| interval | distinctness, order, meaningful differences |
| ratio    | above all                                   |



### differences between Discrete and Continuous Attributes

| Discrete                                                  | Continuous                                                   |
| --------------------------------------------------------- | ------------------------------------------------------------ |
| only finite or countably infinite set of values           | real numbers                                                 |
| often represented as integer variables(binary attributes) | can only be measured and represented using a finite number of digits |
|                                                           | typically represented as floating-point variables            |



### Outliers

outliers are data objects with characteristics that are considerably different than most of the other data objects in the data.



### Missing values

#### reason

because information is not collected, or attributes may not be applicable to all cases, or sometimes may caused by human mistakes.



#### handling missing values

- eliminate data objects or variables
- estimate missing values
- ignore the missing value during analysis



- MCAR
- MAR
- MNAR



### Duplicate Data

#### when

Major issue when merging data from heterogeneous sources

#### How to do

process of dealing with duplicate data issues



### 5 num summary

- min
- Q1
- median
- Q3
- max



### Aggregation

ç”Ÿæˆæ•°æ®

combine two or more attributes into a single attribute

### Sampling

#### reason

too expensive or time consuming to obtain the entire data of interest, so we want to find a representative sample.



#### type

- simple random sampling
  - sampling without replacement
  - sampling with replacement
- stratified sampling

### Dimensionality Reduction

#### Reason

- to avoid curse of dimensionality
- reduce the time and requirement of data mining alogrithms
- make data easily to visualize
- may help to eliminate irrelevant fetures or reduce noise

#### Techniques

- PCA
- SVD

### Feature subset selection

also is a method to reduce dimensionality of data

we can apply feature subset selection to drop redundants features and ittrlevant features

â€‹	

### feature creation

to create new attributes that can capture the important information in a data set much more efficiently than the original attributes. 

#### methodologies:

- feature extraction
- feature construction
- mapping data to new space(kernel method)



### discretization and binarization

- discretization is the process of converting a continuous attribute into an ordinal attribute
- binarization maps a continuous or categorical attribute into one or more binary variables

### attribute transformation

is a function to map the entire set of valuse of a given attribute to a new range.

#### method:

- normalization
- standardizaiton



## Frequent Itemsets

### what is frequent itemsets?

given a support threshold $$s$$, then sets of items that appear in at least s baskets are called frequent itemsets.



**support:** for itemset I , the number of baskets containing all items in I.

#### Rules

**confidence:**is the probability of $$j$$ given $$I = {i_1,...,i_k}$$ï¼Œit used to find the significant/interesting ones in the association rules.

$$conf(I\rightarrow j) = \frac {support(I \cup j)}{support(I)}$$

> but not all high-confidence rules are interesting, because sometimes the item j is very often and independent of i



**Interest** of an association rule $$I \rightarrow j$$:difference beetween rules' confidence and the fraction of baskets that contain j

$$Interest(I \rightarrow j)=conf(I \rightarrow j)-Pr[j]$$



#### Rule generation

**Output**: the rules above the confidence threshold

- variant 1: Single pass to compute the rule confidence
- variant 2: Observation, If $$A,B,C \rightarrow D$$ is below confidence, so is $$A,B \rightarrow C,D$$
- also can generate rules with bigger value of confidence from smaller ones. 



#### Maximal frequent itemset 

æ²¡æœ‰ç›´æ¥è¶…é›†æ˜¯frequentçš„

can gives more pruning



#### Closed frequent itemset

æ²¡æœ‰ç›´æ¥è¶…é›†å’Œå½“å‰itemsetå…·æœ‰ç›¸åŒçš„frequency

stores not noly frequent information, but exact counts



#### comparison between maximal and closed itemsets:

both of them are to reduce the number of rules, we can post-process them and only store output.

no easy to understand

but maximal frequent itemsets can provide more pruning

closed itemsets not noly store the frequent information, but exact counts.



#### Attention æ³¨æ„äº‹é¡¹

åœ¨å¯»æ‰¾æ‰€æœ‰çš„frequent itemsetsä¸­ï¼Œæˆ‘ä»¬ä¸éœ€è¦ç”¨åˆ°confidenceã€liftå’ŒInterestï¼Œåªéœ€è¦ç”¨åˆ°supportå³å¯ã€‚



given d items, there are $$2^d$$ possible candidate itemsets.



### Naive alogrithm

two approaches:

- triangular matrix, cost 4 bytes per pair
- triples, cost 12 bytes per occurring pair

#### comparison between the two approaches

triangular matrix only count pair of items {i,j} if i<j, and it keep pair counts in lexicographic order. Pair {i,j} is at position (i-1)(n-i/2)+j-1, the total number of pairs $$\frac {n(n-1)}{2}$$, and total bytes is $$2n^2$$

and triples only stores the pairs which count > 0, and it beats approach 1 if less than 1/3 of possible pairs actually occur.



### Apriori

first of all, concentrate on pairs, then extend to larger sets.

- if an itemset is frequent, then all of its subsets must also be frequent



#### support counting 

- hash function
- max leaf size

to reduce number of comparisons, store the candidate itemsets in a hash structure



#### å½±å“Apriori å¤æ‚åº¦çš„å› ç´ 

- choice of minimum support threshold 
  - è¾ƒä½çš„thresholdä¼šå¯¼è‡´æ›´å¤šçš„frequent itemsetsäº§ç”Ÿï¼Œè¿™ä¼šå¯¼è‡´candidatasæ•°å’Œfrequent itemsetsçš„é•¿åº¦å¢åŠ 
- dimensionality(number of items) of the dataset
  - éœ€è¦æ›´å¤šçš„å†…å­˜ç©ºé—´å»å­˜å‚¨æ¯ä¸ªitemçš„support countï¼Œè¿™æ ·åœ¨scançš„æ—¶å€™ï¼Œéœ€è¦æ›´å¤šçš„computation å’ŒI/O costs
- size of database
  - transactions çš„æ¬¡æ•°å¤šäº†
- average transaction width
  - may increase  max length of frequent itemsets and traversals of hash tree.



### FP-Growth



### Comparsion

**Advantages of Apriori:**

- easy to get frequent sets

**Disadvantages of Apriori:**

- breadth-first search
- candidate generation and test, which means that it might generate a huge number of candidates, and need to scan the transaction many times.



**Advantages of FP-Growth:**

- just scan the database once to store all essential information in a data structure called FP-tree
- divide-and-conquer
  - depth-first search

- Completeness
  - preserve complete information for frequent pattern mining
  - never break a long pattern of any transaction, it grow long patterns from short ones using local frequent items only
- Compactness
  - reducue irrelevant info, and avoid explicit candidate generation
  - items in frequency descending order: the more frequently occurring, the more likely to be shared
  - never be larger than the original database.

**Disadvantages of FP-Growth:**

- if there are too many items, it takes time to calculate and find the frequent itemsets.



## LSH

goal: find the similar items



### Shingling

å°†documents å‘é‡åŒ–

k-shingle åˆç§°ä¸ºk-gram,

å³å°†documentæ‹†åˆ†æˆæ¯ä¸ªå¤§å°ä¸ºkçš„è¡¨ç¤ºæ–¹æ³•

å½“kéå¸¸å¤§çš„æ—¶å€™ï¼Œæ¯ä¸€ä¸ªsingleéƒ½å¾ˆé•¿ï¼Œä¸åˆ©äºå­˜å‚¨ï¼Œå¯ä»¥hashä»–ä»¬

<img src="/Users/Simonchan/Library/Application Support/typora-user-images/image-20200518165052496.png" alt="image-20200518165052496" style="zoom:50%;" />

#### similarity metric for shingles

- å¯¹æ¯ä¸ªæ–‡ä»¶ç”¨ä¸€ä¸ª01å‘é‡æ¥è¡¨ç¤º,ä½†æ˜¯è¿™æ ·çš„è¡¨ç¤ºæ–¹æ³•ä¼šå¯¼è‡´ç¨€ç–
- å¯ä»¥é€šè¿‡jaccard similarity æ¥è®¡ç®—æ–‡ä»¶é—´çš„ç›¸ä¼¼åº¦



#### kå€¼çš„é€‰å–

- k=5, çŸ­æ–‡ä»¶
- k=10, é•¿æ–‡ä»¶

### Min-Hashing

æ¯ä¸ªæ–‡ä»¶å‘é‡ä¸¤ä¸¤å¯¹æ¯”è®¡ç®—é‡å¤ªå¤§ï¼Œè€Œä¸”æµªè´¹æ—¶é—´ï¼Œæ‰€ä»¥å¼•å…¥min-hashingè§£å†³

é€šè¿‡å‘é‡è¡¨ç¤ºä¸¤ä¸ªæ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œä¸¤ä¸ªæ–‡ä»¶ä¸­éƒ½æœ‰çš„termç”¨1è¡¨ç¤ºï¼Œæ²¡æœ‰çš„è¯ç”¨0è¡¨ç¤ºã€‚

ç„¶åå°†æ‰€æœ‰å‘é‡æ•´åˆåœ¨ä¸€èµ·åï¼Œä»¥ä¸¾è¯å½¢å¼è¡¨ç¤ºï¼Œå…¶ä¸­æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªterm(shingle), æ¯ä¸€åˆ—è¡¨ç¤ºä¸€ä¸ªdocument



#### signatures

ç”¨ç­¾åç›¸ä¼¼åº¦ä»£è¡¨æ–‡ä»¶ç›¸ä¼¼åº¦ï¼Œä»è€Œå‹ç¼©æ–‡ä»¶å¤§å°

![image-20200518171315802](/Users/Simonchan/Library/Application Support/typora-user-images/image-20200518171315802.png)



å…·ä½“å®ç°ï¼š

- ç”¨$$\pi$$åºåˆ—è·å–signature matrix

<img src="/Users/Simonchan/Library/Application Support/typora-user-images/image-20200518171510725.png" alt="image-20200518171510725" style="zoom:50%;" />

è¿™ä¸ªsignature matrix å°±æ˜¯é€šè¿‡æ¯ä¸€åˆ—çš„$$permutation\space \pi$$å»æ‰¾åˆ°æ¯ä¸€ä¸ªdocumentä¸­ç¬¬ä¸€ä½ä¸ä¸ºé›¶çš„åœ¨åºåˆ—ä¸­çš„ç¼–å·



- é€šè¿‡signature matrixå»è®¡ç®—similarity

  <img src="/Users/Simonchan/Library/Application Support/typora-user-images/image-20200518172434609.png" alt="image-20200518172434609" style="zoom:33%;" />

å¯ä»¥é€šè¿‡100byteçš„å¤§å°æ¥ä¿å­˜ä¸€ä¸ªpermutationè¿‡åçš„æ–‡ä»¶ï¼Œæ¯ä¸€ä¸ªpermutationå¯¹åº”çš„æœ€å¤§å€¼æ˜¯255,å³2çš„8æ¬¡æ–¹,è¿›è¡Œ100æ¬¡permutation

#### permutation's implementation trick

ç”¨Universal hashingé€‰æ‹©éšæœºå€¼ä»£è¡¨ä½ç½®ï¼Œå»æœç´¢ä¸€ä¸ªæ–‡ä»¶çš„è¿™è¡Œæ˜¯å¦æœ‰1ï¼Œè‹¥æœ‰1å°±ç”¨è¿™ä¸ªå€¼ä½œä¸ºè¿™æ¬¡min hashingçš„ç»“æœ



### Locality-Sensitive Hashing

Goal: find documents with Jaccard similarity at least s



#### Step

1. é¦–å…ˆæŠŠæ¯ä¸€ä¸ªæ–‡ä»¶åœ¨signature matrixä¸­åˆ†æˆr*bçš„æ ¼å­,**b=bandä¸ªæ•°ï¼Œr=æ¯ä¸ªbandçš„è¡Œæ•°**

   <img src="/Users/Simonchan/Library/Application Support/typora-user-images/image-20200518173029305.png" alt="image-20200518173029305" style="zoom:30%;" />

2. å†é€šè¿‡æŸä¸€ç§å“ˆå¸Œæ–¹å¼æŠŠæ¯ä¸ªæ–‡ä»¶çš„åŒä¸€ä¸ªbandä¸¢è¿›bucketsä¸­ï¼Œä¾æ¬¡å¯¹æ¯ä¸€ä¸ªbandè¿›è¡Œè¯¥æ“ä½œ

   <img src="/Users/Simonchan/Library/Application Support/typora-user-images/image-20200518173128126.png" alt="image-20200518173128126" style="zoom:33%;" />

3. åªè¦ä¸¤ä¸ªæ–‡ä»¶çš„ä»»æ„ä¸€ä¸ªbandåœ¨åŒä¸€ä¸ªbucketä¸­ï¼Œè¿™ä¸¤ä¸ªæ–‡ä»¶å°±æˆä¸ºcandidate pair,å³é˜€å€¼è¶…è¿‡äº†sçš„æ–‡ä»¶å¯¹ï¼Œåä¹‹å°±ä¸æ˜¯candidate pair

#### åŸç†

**simplifying assumption:**æœ‰è¶³å¤Ÿå¤šçš„bucketsï¼Œä»¥è‡³äºåªæœ‰åœ¨å®Œå…¨ç›¸åŒçš„bandæ‰èƒ½å¤Ÿæ‰åˆ°åŒä¸€ä¸ªbucket

å¯¹äºä¸åŒçš„bï¼Œrå’Œä¸¤ä¸ªdocumentçš„similarityå€¼tï¼Œè‡³å°‘æœ‰ä¸€ä¸ªbandæ‰åœ¨åŒä¸€ä¸ªbucketçš„æ¦‚ç‡

<img src="/Users/Simonchan/Library/Application Support/typora-user-images/image-20200518173416451.png" alt="image-20200518173416451" style="zoom:33%;" />

#### examples

- ä¸¤ä¸ªæ–‡ä»¶çš„similarityçš„t=0.8ï¼Œé˜€å€¼s=0.8ï¼Œb=20ï¼Œr=5ã€‚å°†ä¼šæœ‰0.99965çš„æ¦‚ç‡è¿™ä¸ªæ–‡ä»¶å°†ä¼šè¢«æ­£ç¡®åœ°é¢„æµ‹ï¼Œå³è‡³å°‘ä¸€ä¸ªbandä¼šæ‰å…¥åŒä¸€ä¸ªbucket
- ä¸¤ä¸ªæ–‡ä»¶çš„similarityçš„t=0.3ï¼Œé˜€å€¼s=0.8ï¼Œb=20ï¼Œr=5ã€‚å°†ä¼šæœ‰0.0474çš„æ¦‚ç‡è¿™ä¸ªæ–‡ä»¶å°†ä¼šè¢«é”™è¯¯åœ°é¢„æµ‹ä¸ºç›¸ä¼¼æ–‡ä»¶å¯¹ï¼Œå³è‡³å°‘ä¸€ä¸ªbandä¼šæ‰å…¥åŒä¸€ä¸ªbucket

### summary

- bè¶Šå°ï¼Œrè¶Šå¤§ï¼Œä¸¤ä¸ªæ–‡ä»¶è‡³å°‘æœ‰ä¸€ä¸ªbandæ‰è¿›åŒä¸€ä¸ªbucketçš„æ¦‚ç‡å°±å°ï¼Œfalse positiveçš„æ¦‚ç‡å°±å˜å°ï¼Œfalse negativeçš„æ¦‚ç‡å°±å˜å¤§

- ç†æƒ³çŠ¶æ€çš„LSH,å³è¶…è¿‡é˜€å€¼å³ä¸€å®šé¢„æµ‹ä¸ºcandidate pair



## Clustering

to find groups of objects such that the objects in a group will be similar(or related) to one another and different from(or unrelated to) the objects in other groups.

### Types of clusters:

- well-separated 
- center-based
- contiguity-based(nearest neighbor or transitive)
- density-based
- conceptual(æœ‰overlappingçš„)
- objective function- ç”±ç›®æ ‡å‡½æ•°å†³å®š



### K-means

> å°†æ‰€æœ‰ç‚¹åˆ†é…å¥½åï¼Œå†æ›´æ–°ä¸­å¿ƒç‚¹

#### limitation

k-means has problems when clusters are of differing:

- sizes
- densities
- non-globular shpaes

it also cannot figure out the problem when data contains outliers

k-means çš„åˆå§‹ç‚¹é€‰æ‹©å¾ˆé‡è¦ï¼Œä¸€ä¸ªå¥½çš„åˆå§‹ç‚¹å¯ä»¥æ›´å¿«æ”¶æ•›ï¼Œä¸”äº§ç”Ÿæ›´å¥½çš„clusterï¼›å¦‚æœåˆå§‹ç‚¹é€‰æ‹©ä¸å¥½ï¼Œä¼šä½¿clusteringçš„æ—¶é—´å’Œè®¡ç®—é‡åŠ å¤§ã€‚

#### é€‰æ‹©åˆå§‹ç‚¹

å¦‚æœæœ‰kä¸ªclustersï¼Œé‚£ä¹ˆä»å…¶ä»–clusterä¸­é€‰æ‹©åˆ°ä¸€ä¸ªä¸­å¿ƒçš„æœºä¼šæ˜¯å¾ˆå°çš„ï¼Œå°¤å…¶æ˜¯å½“kå¾ˆå¤§çš„æ—¶å€™ã€‚å‡è®¾clusteréƒ½å…·æœ‰ç›¸åŒçš„size=n:

$$P =\frac{K!n^K}{(Kn)^K} = \frac{K!}{K^K}$$



#### Solutions to initial centroids problem

- multiple runs
- sample and use hierarchical clustering to determine initial centroids
- select more than k initial centroids and then select among these initial centroids
- postprocessing
- generate a larger number of clusters and then perform a hierarchical clustering
- bisecting k-means



#### Solutions to Empty Clusters problem

- choose the point that contributes most to SSE
- choose a point from the cluster with the highest SSE
- if there are several empty clusters, the aboce can be repeated several times.



#### Updating Centers Incrementally 

æ¯æ·»åŠ ä¸€ä¸ªç‚¹ï¼Œå°±æ›´æ–°ä¸€æ¬¡ä¸­å¿ƒ



#### Pre-processing & Post-processing

- pre-processing
  - normalization
  - eliminate outliers
- post-preprocessing
  - eliminate small clusters that may represent outliers
  - split 'loose' clusters(æ‹¥æœ‰è¾ƒé«˜SSEçš„)
  - merge clusters(è¾ƒä½SSEå’Œæ¯”è¾ƒæ¥è¿‘çš„)
  - ISODATA



K-meanså¯¹outliersæ•æ„Ÿ



**ä¼˜ç‚¹ï¼š**

1. easy to implement with simple principle 
2.  If the k value and the initial center value are appropriate, the clustering results are better

**åå¤„ï¼š**

1. The number of K cannot be determined
2. It is sensitive to outliers
3. The algorithm complexity is not easy to control, and the number of iterations may be more
4. Local optimal solution rather than global optimal solution (this is related to the initial selection)
5. Unstable results (influenced by input order)
6. Cannot be incrementally calculated
7. Sometimes may lead to empty clusters
8. the result depends on K and initials



### K-medoidsï¼ˆPAM, partition around medoidsï¼‰

å’Œk-meansç›¸ä¼¼ï¼Œä¸è¿‡ä¸è®¡ç®—å‡å€¼ï¼Œè€Œæ˜¯é‡‡ç”¨äº¤æ¢the most centrally located pointsä½œä¸ºæ–°çš„ä¸­å¿ƒ.

<img src="/Users/Simonchan/Library/Application Support/typora-user-images/image-20200518192735503.png" alt="image-20200518192735503" style="zoom:33%;" />



éœ€è¦è®¡ç®—æ¯ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œç„¶åé€‰æ‹©æœ€ç»ˆclusteringåï¼Œsseæœ€å°çš„

### Hierachical clustering

å±‚æ¬¡èšç±»ï¼Œä¸¤ç§ç±»å‹ï¼šAgglomerativeå’ŒDivisive



**ä¼˜ç‚¹ï¼š**

1. it does not need to make an assumption about the number of clusters

2. They may correspond to meaningful taxonomies

**ç¼ºç‚¹**ï¼š

1. time and space requirements

2. once a decision is made to combine two clusters, it cannot be undone

3. no global objective function is directly minimized

4. different schemes have problems with one or more of the following

5. Breaking large clusters

   

#### Agglomerative

å‡è®¾æ¯ä¸ªç‚¹éƒ½æ˜¯ä¸€ä¸ªclusterï¼Œç„¶åè®¡ç®—clusterä¹‹é—´æœ€å°è·ç¦»çš„ä¸¤ä¸ªï¼Œç„¶åå°†å…¶åˆå¹¶ï¼Œå¹¶æ›´æ–°è·ç¦»çŸ©é˜µã€‚



å››ç§é€‰æ‹©è·ç¦»:

- min(single link)
- max(complete linkage)
- group average
- distance between centroids



#### MIN or Single Link

ä¼˜ç‚¹ï¼šå¯ä»¥å¤„ç† non-elliptical shapes

ç¼ºç‚¹ï¼šå¯¹noise å’Œoutliersæ•æ„Ÿ



#### MAX or Complete LInk

ä¼˜ç‚¹ï¼šnoiseå’Œoutliersçš„å½±å“å°‘äº†

ç¼ºç‚¹ï¼šä¼šæŠŠå¤§clusteræ‹†åˆ†ï¼Œå¯¹globular clusteræœ‰åå·®





### MSTï¼ˆMinimum Spanning Treeï¼‰

![image-20200518200220817](/Users/Simonchan/Library/Application Support/typora-user-images/image-20200518200220817.png)

### DBSCAN

åŸºäºå¯†åº¦çš„ç®—æ³•

ä¼˜ç‚¹ï¼š

1. clustering fast
2.  can effectively process noise points and find spatial clustering of arbitrary shapes
3. The shape of the cluster is not biased
4. the number of clusters does not depend on initialization requirements

åå¤„ï¼š

1. When the data volume increases, it requires a large amount of memory support and I/O consumption is also large
2.  When the density of spatial clustering is not uniform and the spacing difference between clustering is large, the clustering quality is poor, because the parameters MinPts and Eps are difficult to select in this case
3. Algorithm clustering effect depends on and distance formula selection. In practice, Euclidean distance is often used. For high-dimensional data, there is a "dimension disaster".





## Classification

> ä»¥ä¸‹ä¸‰ç§ç®—æ³•éƒ½æ˜¯é€‰æ‹©Gainæœ€å¤§çš„ã€‚

![image-20200518211047204](/Users/Simonchan/Library/Application Support/typora-user-images/image-20200518211047204.png)

### Decision tree based classification

**advantages:**

- inexpensive to construct
- extremely fast at classifying unknown records
- easy to interpret for small-sized trees
- robust to noise, especially when methods to avoid overfitting are employed, pruning can help to reduce  the influence of the noise point
- can easily handle redundant or irrelevant attributes, unless there are some relationship between the attributes

**disadvantages:**

- space of possible decision trees is exponentially large, and greedy approaches can not find the best tree every times.
- does not take into account interactions between attributes
- each decision boundary involves only a single attribute



## Recommendation System

### Content-Based

**Pros:**

1. No need for data on other users(no cold-start or sparsity problems)

2. able to recommend to users with unique tastes

3. able to recommend new & unpopular items(no first-rater problem)

4. able to provide explanations(Can provide explanations of recommended items by 

   listing content-features that caused an item to be 

   recommended)

**Cons:**

1. finding the appropriate features is hard, we have no idea which contents are users' preference.
2. recommendations for new users, we don't know how to construct the user profile
3. overspecialization, the item which recommends to user are common





### Collaborative Filtering

Harnessing quality judgments of other users



å…³äºç›¸ä¼¼åº¦è®¡ç®—å…¬å¼çš„é—®é¢˜ï¼š

1. Jaccard similarity measure

   **problems:** ignores the value of the rating

2. Cosine similarity measure

   **problems:** treats missing ratings as "negative", in another word, just consider the item which two rates  exist.

3. Pearson correlation coefficient



å¦‚æœæƒ³è®©sim(a,b)æ’å¤§äºsim(a,c),å¯ä»¥è®©rateå‡å»row_meanï¼Œå¦å¤–ï¼Œç”¨pearsonå»ç®—çš„æ—¶å€™ï¼Œå‡å»çš„å¹³å‡å€¼æ˜¯å·²ç»é¢„æµ‹è¿‡çš„å¹³å‡å€¼ï¼Œè€Œéç®—ä¸Šæœªé¢„æµ‹è¿‡çš„æ•°ã€‚



### Rating prediction

å¯¹äºUser-User æ¥è¯´ï¼Œçœ‹çš„æ˜¯ä¸å½“å‰ç”¨æˆ·æœ€ç›¸ä¼¼çš„å¹¶ä¸”å¯¹item iè¿›è¡Œäº†è¯„åˆ†çš„kä¸ªç”¨æˆ·ï¼Œæ±‚è¿™äº›ç”¨æˆ·å¯¹içš„å¹³å‡è¯„åˆ†

$$r_{xi}=\frac {\sum_{y\in N}s_{xy}r_{yi}}{\sum_{y\in N}s_{xy}}$$

è€Œå¯¹äºItem-Itemæ¥è¯´ï¼Œæˆ‘ä»¬çœ‹çš„æ˜¯å’Œå½“å‰itemç›¸ä¼¼çš„ä¸”è¢«å½“å‰ç”¨æˆ·è¯„åˆ†äº†çš„itemï¼Œæ±‚ç”¨æˆ·å¯¹è¿™äº›ç›¸ä¼¼äº§å“çš„å¹³å‡è¯„åˆ†

$$r_{xi}=\frac {\sum_{j\in N(i;x)}s_{ij}r_{xj}}{\sum_{j\in N(i;x)}s_{ij}}$$



**pros:**

1. Works for any kind of item, because no feautre selection needed



**cons:**

1. cold start problem, it need enough users in the system to find a match
2. sparsity, it is hard to find users that have rated the same items
3. first rater, it cannot recommend an item that has not been previously rated, such as new items
4. popularity bias, it cannot recommend items to someone with their own favourite, and it tends to recommend popular items because of high similarity.





### Hybrid methods

1. æ··åˆæ³•ï¼Œé‡‡ç”¨ä¸¤ä¸ªæˆ–å¤šä¸ªä¸åŒçš„æ¨èæ–¹æ³•ï¼Œç„¶åå»ç»“åˆé¢„æµ‹(implenment two or more different recommenders and combine predictions)
2. å°†content-based methodsç”¨åˆ°cfä¸Š(add content-based methods to collaborative filtering)





### Latent Factor Model

å°±æ˜¯é‡‡ç”¨äº†SVDé™ç»´ã€‚

ä¼ ç»Ÿçš„SVDæ˜¯$$A = U\sum V^T$$

è¿™é‡Œä»¤$$P = \sum V^T$$,æ‰€ä»¥$$A=U \cdot P^T$$

> æ³¨æ„ï¼Œè¿™é‡Œçš„svdéœ€è¦æ’åºè®¡ç®—

ç„¶åè¦é¢„æµ‹è¯„åˆ†ï¼Œåˆ™æŠŠAçŸ©é˜µä¸­çš„è¡Œå’Œåˆ—æå–å‡ºæ¥ï¼Œç„¶åå†Uä¸­æå–è¯¥è¡Œçš„å€¼ï¼Œå¹¶ä¸$$P^T$$ä¸­çš„åˆ—ç›¸ä¹˜ï¼Œå³æ˜¯æˆ‘ä»¬çš„é¢„æµ‹å€¼ã€‚



SSEå°±æ˜¯æœ€å°åŒ–åŸçŸ©é˜µå’Œè¿˜åŸçŸ©é˜µçš„å·®ã€‚

ä½†æ˜¯è¯„åˆ†çŸ©é˜µä¸­ï¼Œæ–°ç”¨æˆ·ä¼šæ²¡æœ‰é¢„æµ‹å€¼ï¼Œæ‰€ä»¥éœ€è¦ä¸€ä¸ªæ–°çš„æ–¹æ³•å»æ‰¾åˆ°Uå’ŒP



### Evaluating predictions

- æ¯”è¾ƒå·²çŸ¥çš„ratingsï¼š
  - RMSE- might penalize a method that does well for high ratings and badly for others.
  - Precision aat top 10
  - Rank Correlation 

- 0/1 model

  - coverage

  - precision

  - Receiver operatiing characteristic(ROC)

    

### Problems with Error Measure

Narrow focus on accuracy sometimes misses the point such as prediction diversity, prediction context and order of predictions

äº‹å®ä¸Šï¼Œæˆ‘ä»¬åªå…³æ³¨é«˜åˆ†çš„ã€‚

## Big Graph Data Processing

data preprocessing å’Œ data analysis çš„å…³ç³»ï¼Œæ•°æ®é‡è¶Šæ¥è¶Šå¤§ï¼Œåˆ†æè¶Šæ¥è¶Šéš¾ï¼Œå·¥å…·å’Œç®—æ³•ç­‰æ–¹é¢å—åˆ°äº†é™åˆ¶ï¼Œæ‰€ä»¥ä¸€ä¸ªå¥½çš„é¢„å¤„ç†æ–¹å¼å¯ä»¥æé«˜æ•°æ®åˆ†æçš„è´¨é‡ã€‚



### Graph Models

**primary key:** the value of the primary key can identify an entity uniquely. ä¸»é”®å¿…é¡»å…·æœ‰å¯è¯†åˆ«æ€§å’Œå”¯ä¸€å€¼ã€‚ 

 Graph ğº(ğ‘‰, ğ¸) 

- V is the set of vertices, E is the set of edges.

å¯ä»¥é€šè¿‡é‚»æ¥çŸ©é˜µå»è¡¨ç¤ºä¸€ä¸ªå›¾ï¼Œè¡Œè¡¨ç¤ºå‡ºå‘ç‚¹ï¼Œåˆ—è¡¨ç¤ºåˆ°è¾¾çš„ç‚¹æœ‰å‘å›¾å’Œæ— å‘å›¾çš„è¡¨ç¤ºæ–¹æ³•ç¨å¾®æœ‰äº›ä¸åŒï¼Œæ— å‘å›¾æ˜¯ä¸€ä¸ªå¯¹ç§°çŸ©é˜µï¼Œè€Œæœ‰å‘å›¾ä»$$a \rightarrow b$$çš„æ—¶å€™ï¼Œæ‰è¡¨ç¤º1ã€‚

**Advantages of adjacent matrix**

- Amenable to mathematical manipulation.

- Iteration over rows and columns corresponds to computations on outlinks and inlink. 

**Disadvantages of adjacent matrix:**

- Lots of zeros for sparse matrices. Lots of wasted space.



> All the massive graphs are sparse. 



å¦å¤–çš„å›¾è¡¨ç¤ºæ³•ï¼š **Adjacent lists**ï¼ŒæŠŠé‚»æ¥çŸ©é˜µä¸­æ‰€æœ‰é0çš„ç‚¹å»æ‰ï¼Œåªä¿ç•™outlinksã€‚

**Advantages:** 

- Compact representation

- Easy to compute outlinks

**Disadvantages:**

- Cost more to compute inlinks



#### Common friends

ä¸¤ä¸ªç‚¹ä¹‹é—´çš„å…±åŒå¥½å‹æ•°å°±æ˜¯ä¸¤ä¸ªç‚¹ç›¸è¿çš„è¾¹çš„å…¬æœ‰ä¸‰è§’å½¢æ•°ã€‚



#### Degree

degreeå°±æ˜¯å›¾çš„é¡¶ç‚¹ä¸ªæ•°æ‰€åŒ…å«çš„è¾¹æ•°ï¼Œå¦‚æœæœ‰è‡ªå·±æŒ‡å‘è‡ªå·±çš„å¾ªç¯ï¼Œé‚£è¿™ä¸ªdegreeå°±æ˜¯2



### Community Related Algorithms

Community çš„ç‰¹ç‚¹ï¼šdense, connected, possibly additional constraints.



#### K-core

k-core measures whether the subgraph is densely connected.

k-coreæ˜¯å­å›¾ä¸­çš„æ¯ä¸ªç‚¹çš„degreeéƒ½å¤§äºç­‰äºkã€‚



**Property of k-core:** 

- Each kâ€™-core is contained in some k-core if kâ€™>k.

- Every vertex in a k-core has degree at least k.



#### A naibe algorithm about finding k-core:

- å¤šæ¬¡è¿­ä»£ï¼Œå°†$$degree \lt k$$çš„ç‚¹ç§»é™¤ï¼Œç›´åˆ°ä¸å†ç¼©å°

time complexity: $$ O(N^2)$$

#### ä¼˜åŒ–çš„ç®—æ³•ï¼šbucket-based algorithm

Bucket[i]: keeps the node u if degree(u)=i

æ­¥éª¤ï¼š

do{:

- Remove the node ğ‘¢ if degree(u)<k 
  and ğ‘¢ has the minimum current degree;
- Move the node ğ‘£ up by a bin in the 
  bucket if ğ‘£ âˆˆ ğ‘›ğ‘ğ‘Ÿ(ğ‘¢) 

}while(buckets[0-k-1] is empty)

time complexity: $$O(N)$$



#### Collapsed k-core

å³ç§»é™¤æŸä¸ªç‚¹åçš„å›¾ï¼Œä¸è¯¥ç‚¹ç›¸å…³çš„edgeéƒ½ä¼šç§»é™¤ã€‚

Given a graph ğº(ğ‘‰, ğ¸) and a set of vertices ğ´ âŠ† ğ‘‰, the collapsed k-core, denoted by $$C_k(G_A)$$, is the corresponding k-core of ğº with vertices in ğ´ removed.

ç›®æ ‡æ˜¯ä½¿$$C_k(G_A)$$è¿™ä¸ªsubgraphçš„sizeæœ€å°



### Link Analysis

#### find single-source shortest paths

algorithm:

1. åˆå§‹åŒ–æ‰€æœ‰çš„ç‚¹ï¼Œèµ·å§‹ç‚¹ä¸º0ï¼Œå…¶ä»–ç‚¹ä¸º$$\infin$$
2. ç„¶åé€‰æ‹©è·ç¦»æœ€è¿‘çš„é‚£ä¸ªç‚¹ï¼Œå¹¶é‡æ–°è®¡ç®—è¯¥ç‚¹ä¸å…¶ä»–ç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œå¦‚æœæ¯”ä¹‹å‰å®šä¹‰çš„å°ï¼Œåˆ™æ›´æ–°ï¼›å¦åˆ™ä¸å˜
3. é‡å¤æ­¥éª¤2ï¼Œç›´åˆ°Q={}

#### Ranking the results of a query

| Scores                  | Feature    | Example  |
| ----------------------- | ---------- | -------- |
| Query independent score | importance | PageRank |
| Query dependent score   | relevance  | HITS     |



### PageRank

- ç»™æ¯ä¸ªé¡µé¢çš„è¶…é“¾æ¥çš„elementèµ‹äºˆä¸€ä¸ªæƒé‡ï¼ŒPageRank of E and denoted by PR(E)
- Backlinks as votes, the more votes, the more imporant the page is.

å¹¶ä¸æ˜¯è¶Šå¤šäººæŠ•ç¥¨å°±è¶Šå‰å®³ï¼Œè€Œæ˜¯ç€é‡äºæ›´æœ‰åœ°ä½çš„äººç»™ä½ æŠ•ç¥¨ã€‚æ¯”å¦‚ä¸€ç¯‡è®ºæ–‡ï¼Œè¢«é˜¿çŒ«é˜¿ç‹—å¼•ç”¨å¾ˆå¤šæ¬¡ï¼Œä¸ä»£è¡¨è¿™ç¯‡è®ºæ–‡å¾ˆé‡è¦ï¼Œä½†æ˜¯å¦‚æœæœ‰å¤§ä½¬å¼•ç”¨äº†è¿™ç¯‡è®ºæ–‡ï¼Œå°±è¯´æ˜è¿™ç¯‡è®ºæ–‡æœ‰ç‚¹é‡è¦äº†ã€‚



- å¦‚æœæœ‰è¶Šå¤šçš„linkså°±è¡¨ç¤ºè¶Šé‡è¦
- å°†in-linkså½“åšvotes
- å¹¶ä¸æ˜¯æ‰€æœ‰linkséƒ½ä¸€æ ·çš„ï¼Œè¶Šæ¥è‡ªè¶Šé‡è¦çš„pageå°±æ‹¥æœ‰æ›´é«˜çš„å€¼



$$F_u$$-set of pages u points to

$$B_u$$-set of pages pointing to u

$$PR(u)=\sum_{v \in B_u} \frac{PR(v)}{|F_v|}$$



ä½†æ˜¯PageRanké™åˆ¶åªèƒ½å¤„ç†small examples, å› æ­¤æˆ‘ä»¬éœ€è¦æ›´å¥½çš„æ–¹æ³•å»å¤„ç†large web-size graphs.

$$r = M \cdot r$$     #å°±æ˜¯WIçš„è¿­ä»£æ–¹æ³•

MçŸ©é˜µæ¯ä¸€åˆ—çš„å€¼ç›¸åŠ ä¸º1ã€‚



æœ‰ä¸¤ä¸ªé—®é¢˜ï¼š

1. dead ends å³ æ²¡æœ‰out-linksï¼Œt'he matrix is not column stochastic so our initial assumptions are not met.
2. spider traps å³ä¸€ä¸ªgroupä¸­çš„æ‰€æœ‰ç‚¹outlinksæ˜¯ä¸€ä¸ªæ­»å¾ªç¯ï¼Œå…¶å®ä¸ç®—æ˜¯problemï¼Œä¸è¿‡å¯¼è‡´æˆ‘ä»¬pr scoreä¸æ˜¯æˆ‘ä»¬è¦çš„ã€‚



#### dead endsçš„è§£å†³æ–¹æ³•ï¼š

teleportï¼ˆredistributionï¼‰ï¼šç»™dead endsçš„ç‚¹æ·»åŠ æŒ‡å‘å…¶ä»–æ¯ä¸ªç»“ç‚¹çš„åº¦ã€‚



#### Spiders traps çš„è§£å†³æ–¹æ³•ï¼š

always-teleport



damping  factorçš„æ–¹æ³•ï¼ˆRandom teleportï¼‰



#### Topic-Specific PageRank

- Evaluate Web pages not just according to their popularity, but by how close they are to a particular topi

- Allows search queries to be answered based on interests of the user

å’Œä¼ ç»Ÿçš„PageRankå·®ä¸å¤šï¼Œä¸è¿‡è¿™é‡Œæ›´å…³æ³¨çš„æ˜¯å…·æœ‰æƒ³è¦topicçš„Set

$$A_{ij} = \frac{1-d}{|S|} + dM_{ij}$$

åœ¨sä¸­çš„æƒé‡éƒ½æ˜¯ä¸€æ ·çš„ï¼Œä¸è¿‡ä¹Ÿèƒ½ç»™ä¸åŒçš„pagesåˆ†é…ä¸åŒçš„æƒé‡ï¼Œæœ€ç»ˆçš„ç»“æœä¾ç„¶ç¨€ç–ï¼ˆsparsenessï¼‰





### HITS(Hypertext-Induced Topic Selection)

Is a measure of importance of pages or documents, similar to PageRank

- å¦‚æœä¸€ä¸ªç«™ç‚¹è¢«å¼•ç”¨å¾ˆå¤šæ¬¡ï¼Œé‚£ä¹ˆå®ƒæ˜¯éå¸¸æƒå¨çš„ï¼Œé‡è¦ç«™ç‚¹çš„å¼•ç”¨æ¯”ä¸é‡è¦ç«™ç‚¹çš„å¼•ç”¨æƒé‡æ›´å¤§
- hubnessæ˜¾ç¤ºäº†ç«™ç‚¹çš„é‡è¦æ€§ï¼Œä¸€ä¸ªå¥½çš„hubæ˜¯ä¸€ä¸ªé“¾æ¥åˆ°è®¸å¤šæƒå¨ç«™ç‚¹çš„ç«™ç‚¹ã€‚



#### mutually recursive definition

- a good hub links to many good authorities
- a good authority is linked from many good hubs

<img src="/Users/Simonchan/Library/Application Support/typora-user-images/image-20200517171215090.png" alt="image-20200517171215090" style="zoom:50%;" />

å½“å¹³æ–¹å·®å°äºä¸€ä¸ªé˜ˆå€¼ï¼Œå³æ”¶æ•›ã€‚

$$a = A^T(Aa)=(A^TA)a$$

$$h = A(A^Th)=(AA^T)h$$



### Comparison between PageRank and HITS

å…±åŒç‚¹ï¼š 

PageRank and HITS are two solutions to the same problem:**What is the value of an in-link from u to v**?

- In the PageRank model, the value of the link depends on the links **into** *u* 

- In the HITS model, it depends on the value of the other links **out of** *u* 

ä¸åŒç‚¹ï¼š

- PageRank computes authorities only. HITS computes both authorities and hubs.

- The existence of dead ends or spider traps does not affect the solution of HITS.





### Map Reduce System(Distributed System)

é‡åˆ°çš„é—®é¢˜ï¼š

- storage
- cannot mine one a single server
- network



åˆ†å¸ƒå¼è®¡ç®—

æ–‡æ¡£è¢«åˆ‡åˆ†åˆ°è¿ç»­çš„å—ä¸­



any distributed system should deal with two task:

- storage
- computation



#### data structures in MapReduce

Key-Value pairsï¼Œæ•°å€¼ç±»å‹å¯ä»¥ä¸ºintegers, float, strings, raw bytes

ä¸è¿‡ä¹Ÿèƒ½ä¸ºå…¶ä»–ä»»æ„çš„æ•°æ®ç»“æ„





#### STEP

1. map step:å°†æ•°æ®å½¢æˆk-vé”®å€¼å¯¹å½¢å¼
2. reduce stepï¼šå°†k-væ ¹æ®kå€¼groupèµ·æ¥ï¼Œç„¶åæ’åºåreduce



#### Failure

å¤±è´¥çš„æƒ…å†µåˆ†ä¸ºmapé˜¶æ®µå¤±è´¥ï¼Œreduceé˜¶æ®µå¤±è´¥å’Œmasteréƒ¨åˆ†å¤±è´¥

- Map worker failure 
  - Map tasks completed or in-progress at worker are reset to idle 
  - Reduce workers are notified when task is rescheduled on another worker 
- Reduce worker failure 
  - Only in-progress tasks are reset to idle 
- Master failure 
  - MapReduce task is aborted and client is notified



### Advanced Topics



![image-20200519204037456](/Users/Simonchan/Library/Application Support/typora-user-images/image-20200519204037456.png)





